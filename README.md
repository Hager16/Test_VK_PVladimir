# Test_VK_PVladimir
Repository is created for supervision of testing task for VK

Тестовое задание было выполнено на арендованном сервере. 

Данные для входа в web-интерфейс Airflow:

http://217.144.184.163:8080 

имя пользователя: surveyor

пароль: TesT1102

В ходе работы был реализован следующий функционал:
1.	Первичная реализация пайплайна на Python:
- Написан скрипт для генерации данных за день (daily_agg.py).
- Написан скрипт для недельной агрегации данных (weekly_agg.py).
- Настроен DAG в Apache Airflow, который:
	Генерирует данные за предыдущий день в 06:50;
	Выполняет агрегацию данных за день в 06:55;
	Выполняет недельную агрегацию в 07:00.
- Созданы директории input (сгенерированные данные), intermediate (агрегированные данные за 1 день), output (агрегированные данные за 7 дней) и input_spark, intermediate_spark, output_spark для тех же задач, но выполняемых на Spark, чтобы избежать конфликта данных.   
2.	Реализация пайплайна с использованием Apache Spark:
- Скрипты для агрегации данных были переписаны на PySpark.
- Выполнялись задачи Spark через оператор BashOperator в Airflow.
- Настроен запуск Spark через bash, так как при попытке запуска через SparkSubmitOperator столкнулся с рядом проблем по взаимодействию Apache Airflow, запущенному через docker.compose, с виртуальным окружением Apache Spark. 
DAG файл оставил в неактивных задачах Airflow для дальнейшей работы по решению данной проблемы. 
3.	Docker и настройки окружения:
- Настроен docker-compose.yaml файл для развертывания Airflow.
- В docker-compose добавлены необходимые volume для монтирования директорий с данными и скриптами.
- Установлен Spark и виртуальное окружение для работы с PySpark.
- Настроены переменные окружения, такие как PYSPARK_PYTHON и SPARK_HOME.
- Решены проблемы с правами доступа к файлам и директориям в контейнере.
4.	Исправление проблем:
- Решены проблемы с недостающими зависимостями, такими как Java для Spark.
- Исправлены ошибки, связанные с правами доступа при установке зависимостей.
- Осуществлен запуск пайплайна с использованием виртуального окружения для PySpark.

Итогом работы является корректно работающий пайплайн как на Python, так и на PySpark с подключением виртуального окружения через BashOperator.

В случае необходимости получения доступа к консоли сервера, прошу написать напрямую для предоставления.
